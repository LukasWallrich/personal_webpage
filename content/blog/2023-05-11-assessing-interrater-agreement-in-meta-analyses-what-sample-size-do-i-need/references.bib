@Article{waffenschmidtSingleScreeningConventional2019,
  title = {Single Screening versus Conventional Double Screening for Study Selection in Systematic Reviews: A Methodological Systematic Review},
  shorttitle = {Single Screening versus Conventional Double Screening for Study Selection in Systematic Reviews},
  author = {Siw Waffenschmidt and Marco Knelangen and Wiebke Sieben and Stefanie B{\"u}hn and Dawid Pieper},
  date = {2019-06-28},
  journaltitle = {BMC Medical Research Methodology},
  shortjournal = {BMC Medical Research Methodology},
  volume = {19},
  number = {1},
  pages = {132},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0782-0},
  url = {https://doi.org/10.1186/s12874-019-0782-0},
  urldate = {2023-05-11},
  abstract = {Stringent requirements exist regarding the transparency of the study selection process and the reliability of results. A 2-step selection process is generally recommended; this is conducted by 2 reviewers independently of each other (conventional double-screening). However, the approach is resource intensive, which can be a problem, as systematic reviews generally need to be completed within a defined period with a limited budget. The aim of the following methodological systematic review was to analyse the evidence available on whether single screening is equivalent to double screening in the screening process conducted in systematic reviews.},
  keywords = {Methodology,Study selection,Systematic reviews},
  file = {/Users/lukaswallrich/Zotero/storage/9QZ33HKR/Waffenschmidt et al_2019_Single screening versus conventional double screening for study selection in.pdf;/Users/lukaswallrich/Zotero/storage/Q3BCKQZR/s12874-019-0782-0.html},
}

@article{mchughInterraterReliabilityKappa2012,
  title = {Interrater Reliability: The Kappa Statistic},
  shorttitle = {Interrater Reliability},
  author = {McHugh, Mary L.},
  year = {2012},
  month = oct,
  journal = {Biochemia Medica},
  volume = {22},
  number = {3},
  pages = {276--282},
  issn = {1330-0962},
  urldate = {2023-05-11},
  abstract = {The kappa statistic is frequently used to test interrater reliability. The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is called interrater reliability. While there have been a variety of methods to measure interrater reliability, traditionally it was measured as percent agreement, calculated as the number of agreement scores divided by the total number of scores. In 1960, Jacob Cohen critiqued use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen's kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty. Like most correlation statistics, the kappa can range from -1 to +1. While the kappa is one of the most commonly used statistics to test interrater reliability, it has limitations. Judgments about what level of kappa should be acceptable for health research are questioned. Cohen's suggested interpretation may be too lenient for health related studies because it implies that a score as low as 0.41 might be acceptable. Kappa and percent agreement are compared, and levels for both kappa and percent agreement that should be demanded in healthcare studies are suggested.},
  pmcid = {PMC3900052},
  pmid = {23092060}
}

@article{kovalEstimatorsKappaexactSmall1996,
  title = {Estimators of Kappa-Exact Small Sample Properties},
  author = {Koval, John J. and Blackman, Nicole J.M.},
  year = {1996},
  month = nov,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {55},
  number = {4},
  pages = {315--336},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/00949659608811773},
  urldate = {2023-05-11},
  langid = {english},
  file = {/Users/lukaswallrich/Zotero/storage/XTUEHZXJ/Koval_Blackman_1996_Estimators of kappa-exact small sample properties.pdf}
}

@article{shanExactOnesidedConfidence2017,
  title = {Exact One-Sided Confidence Limits for {{Cohen}}'s Kappa as a Measurement of Agreement},
  author = {Shan, Guogen and Wang, Weizhen},
  year = {2017},
  month = apr,
  journal = {Statistical Methods in Medical Research},
  volume = {26},
  number = {2},
  pages = {615--632},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/0962280214552881},
  urldate = {2023-05-11},
  abstract = {Cohen's kappa coefficient, , is a statistical measure of inter-rater agreement or inter-annotator agreement for qualitative items. In this paper, we focus on interval estimation of  in the case of two raters and binary items. So far, only asymptotic and bootstrap intervals are available for  due to its complexity. However, there is no guarantee that such intervals will capture  with the desired nominal level 1\textendash{}. In other words, the statistical inferences based on these intervals are not reliable. We apply the Buehler method to obtain exact confidence intervals based on four widely used asymptotic intervals, three Wald-type confidence intervals and one interval constructed from a profile variance. These exact intervals are compared with regard to coverage probability and length for small to medium sample sizes. The exact intervals based on the Garner interval and the Lee and Tu interval are generally recommended for use in practice due to good performance in both coverage probability and length.},
  langid = {english},
  file = {/Users/lukaswallrich/Zotero/storage/IW7HBNZA/Shan and Wang - 2017 - Exact one-sided confidence limits for Cohenâ€™s kapp.pdf}
}
